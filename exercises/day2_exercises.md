# 4. Parallel R

### Ex 13 : foreach

1.  In the course project folder on Puhti `/scratch/project_2016453/shared_data` there is a folder `shared_data` that contains three .csv files. Copy this folder to your personal folder under `/scratch/project_2016453/your_folder/communities`.

2.  Start an RStudio session on the Puhti web interface (www.puhti.csc.fi) using the following resources:

    project: project_2016453

    reservation: high_perf_r_2

    number of CPU cores: 5

    memory: 6 GB

    local disk: 4 GB (default)

    R version: 4.5.1 (default)

    time: 4:00:00 (default)

3.  First, check how long running the following code snippet takes (use one of the timing approaches introduced on day 1).

Note: change the file path in the first command to your folder where you copied the `communities` folder

``` r
# creating a list of .csv files in a folder

comm_csv_list <- list.files(path = "/scratch/project_2016453/xxxxxx/communities/", pattern = ".csv", full.names = TRUE) 

# the for loop below goes through the .csv files in the list and carries out the same operations on each of them (reads in the csv file, carries out a distance-based NMDS ordination, and saves the stress value that describes the reliability of the ordination)

stress_values <- c()

for (comm_csv in comm_csv_list) {
  comm <- read.csv2(comm_csv, row.names = 1)
  nmds <- vegan::metaMDS(comm, trace = FALSE)
  Sys.sleep(5) # added to extend the running time of the small example
  stress_values <- c(stress_values, nmds$stress)
  }
```

4.  Then, let's change the for loop into a parallel approach using `foreach`.

Check the example in the foreach demonstration for a reminder on what needs to be changed to turn the for loop into a parallel foreach operation.

Hints: Which packages do you need to load? How can you tell `foreach` how many cores to use? What needs to be added to make the operation parallel?

What happens to the running time compared to the serial approach above?


# 5. Running R on an HPC cluster

### Ex 14: Serial batch job

1.  Prepare an R script to be run as a batch job: copy the R script below into a plain text file with the file ending .R. For example, you can use the script window in RStudio to prepare the R script and save it in your personal folder under the course project `/scratch` directory.

``` r
# this R script has two sections:

# 1st one prints out some useful basic information of the R session

print(sessionInfo()) # What does this do? 
print(parallelly::availableCores()) # What does this do?
print(Sys.getenv("SLURM_CPUS_PER_TASK")) # What does this do?

# 2nd section runs the same ordination we used in the foreach example on three .csv files
# Instead of a for loop, we use the map() function in the package purrr, which is 
# a tidyverse alternative to apply() functions.

# Change the file path in the next command to your personal folder
comm_csv_list <- list.files(path = "/scratch/project_2016453/personal/<add folder here>/communities/", pattern = ".csv", full.names = TRUE) 

# A function for running the same ordination we used with foreach
ordination_function <- function(comm_csv) {
  comm <- read.csv2(comm_csv, row.names = 1)
  nmds <- vegan::metaMDS(comm, trace = FALSE)
  Sys.sleep(5) # added to extend the running time of the small example
  stress_values <- nmds$stress
}

# Running the function on the file list with the map() function in the package purrr
results <- purrr::map(comm_csv_list, ordination_function)
print(results)

```

2.  Prepare a batch job script (plain text file, file ending .sh). For example, you can open a text file in the script window of RStudio, copy the code below there, and save the file with the ending .sh in the same folder as your R script above.

``` bash
#!/bin/bash -l
#SBATCH --job-name=my_batchjobtest # give your job a name here
#SBATCH --account=project_2016453 # project number of the course project
#SBATCH --output=output_%j.txt
#SBATCH --error=errors_%j.txt
#SBATCH --partition=small
#SBATCH --time=00:05:00 # h:min:sek, this reserves 5 minutes
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=1000
#SBATCH --reservation=high_perf_r_2 # only used during this course

# Load r-env
module load r-env

# Clean up .Renviron file in home directory
if test -f ~/.Renviron; then
    sed -i '/TMPDIR/d' ~/.Renviron
fi

# Specify a temp folder path (add your personal folder here)
echo "TMPDIR=/scratch/project_2016453/<add your folder here>" >> ~/.Renviron 

# Run the R script
srun apptainer_wrapper exec Rscript --no-save myscript.R #use your R script file here
```

3.  Open a login node shell on in the Puhti web interface and navigate to the folder where your R script file and batch job script file are (`cd foldername` moves you into a folder, `..` moves you one step back in the folder structure, `ls -l` shows the files in a folder).

4.  Submit the job to the Slurm batch queue system on Puhti:

``` bash
sbatch my_batch_job.sh
```

To view the status of the job:

``` bash
squeue -u $USER
# or
squeue --me
```

To cancel a submitted job:

``` bash
scancel <job_id>
```

When the job has finished, check the resources it used:

``` bash
seff <job_id>
```

Check the error and output files that should be in the same folder as your R script and batch job script files (for example with `less output_<job_id>.txt` in the login node shell).

The R script should print the stress values in the output file defined in the batch job script. Are the values there?

### Ex 15: Built-in parallelism

In this exercise, we run an example with the package `brms` . From the website of the package (<https://paulbuerkner.com/brms/>):

"*The **brms** package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan. The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses*."

We are using is as an example of a package, where the use of multiple cores is built in. The only things we have to do to make use of multiple cores is reserve them in the batch job script, and set the number of cores in the function call with `cores = n`. Here, we compare model fitting with 1 core vs. 4 cores.

``` r
library(brms)
library(microbenchmark)

# an 'empty' model run first, because compiling takes a while
fit_empty <- brm(count ~ zAge + zBase * Trt + (1|patient),
              data = epilepsy, family = poisson(),
              chains = 0)

# the actual test with different number of cores
brms_results <- microbenchmark(
  
  single_core = {update(fit_empty, recompile = FALSE,
      chains = 4, cores = 1)
    },
  
  multicore = {update(fit_empty, recompile = FALSE,
    chains = 4, cores = 4)
    }, times = 3
)

print(brms_results)
```

Batch job script:

``` bash
#!/bin/bash -l
#SBATCH --job-name=brms # give your job a name here
#SBATCH --account=project_2016453 # project number of the course project
#SBATCH --output=output_%j.txt
#SBATCH --error=errors_%j.txt
#SBATCH --partition=small
#SBATCH --time=00:15:00 # h:min:sek, this reserves 15 minutes
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=5  # this sets the job to have 5 cores (4 + 1 extra)
#SBATCH --mem-per-cpu=2000
#SBATCH --reservation=high_perf_r_2 # only used during this course

# Load r-env
module load r-env

# Clean up .Renviron file in home directory
if test -f ~/.Renviron; then
    sed -i '/TMPDIR/d' ~/.Renviron
fi

# Specify a temp folder path (add your personal folder here)
echo "TMPDIR=/scratch/project_2016453/<add folder here>" >> ~/.Renviron 

# Run the R script
srun apptainer_wrapper exec Rscript --no-save brms.R # your R script file here
```

Submit the batch job with `sbatch` in a login node shell as above. How does adding more cores change the running time? What would you say about the resource use of this example?

### Ex 16: Array jobs

Array jobs are another way to handle embarassingly parallel problems, for example using the same R script to process many files. Instead of submitting multiple jobs, several subtasks are submitted at once as an array job. The resources in the batch job script are for one subtask of the array.

R script for one iteration of the for loop we had above:

``` r
# this lets us access the array number in R (from $SLURM_ARRAY_TASK_ID in the batch job script)
arrays <- commandArgs(trailingOnly = TRUE)

# converting the array number to a numeric value
arrays <- as.numeric(arrays[1])

# listing the csv files in the folder communities
comm_csv_list <- list.files(path = "/scratch/project_2016453/<your folder here>/communities", pattern = ".csv", full.names = TRUE) 

# selecting the file corresponding to the array number from the file list
comm_csv <- comm_csv_list[arrays]

comm <- read.csv2(comm_csv, row.names = 1)
nmds <- vegan::metaMDS(comm, trace = FALSE)
Sys.sleep(5) # added to extend the running time of the small example
print(nmds$stress)
```

Batch job script (note the line `--array`, the different format of the output and error files, and `$SLURM_ARRAY_TASK_ID`in the end of the last line):

``` bash
#!/bin/bash -l
#SBATCH --job-name=my_array_job # name your job here
#SBATCH --account=project_2016453 # project number of the course project
#SBATCH --output=array_job_out_%A_%a.txt # note the different format
#SBATCH --error=array_job_err_%A_%a.txt # note the different format
#SBATCH --partition=small
#SBATCH --time=00:05:00
#SBATCH --ntasks=1
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=1000
#SBATCH --array=1-3 # specific line to array jobs
#SBATCH --reservation=high_perf_r_2 # only used during this course

# Load r-env
module load r-env

# Clean up .Renviron file in home directory
if test -f ~/.Renviron; then
    sed -i '/TMPDIR/d' ~/.Renviron
fi

# Specify a temp folder path (add your personal folder here)
echo "TMPDIR=/scratch/project_2016453/<your folder here>" >> ~/.Renviron 

# Run the R script
srun apptainer_wrapper exec Rscript --no-save my_array_script.R $SLURM_ARRAY_TASK_ID
```

# 6. Future for parallel R

### Ex 17: Multiprocessing with `future`

The package `future` and the family of R packages around it offer lots of possibilities for running R jobs in parallel, often without complicated modifications to sequential scripts. Here, we use the function `future_map` from the package `furrr` to run our distance matrix example in parallel using multiple cores.

R script:

``` r
library(purrr)
library(furrr) # one package of the future family of packages

# We will use the same function as in Ex 14
ordination_function <- function(comm_csv) {
  comm <- read.csv2(comm_csv, row.names = 1)
  nmds <- vegan::metaMDS(comm, trace = FALSE)
  Sys.sleep(5) # added to extend the running time of the small example
  stress_values <- nmds$stress
}

# listing the csv files in the folder communities
comm_csv_list <- list.files(path = "/scratch/project_2016453/<your folder here>/communities", pattern = ".csv", full.names = TRUE) 

# sequential
sequential <- system.time(results <- purrr::map(comm_csv_list, ordination_function))
print(sequential)

# converting the sequential code into parallel with future_map
plan(multicore)
multiprocessing <- system.time(future_map(comm_csv_list, ordination_function))
print(multiprocessing)
```

Batch job script: use a similar batch job script as in Ex 15, but reserve 3 cores and 1 GB of memory per core.

Submit the batch job with `sbatch`. Check the running times of the sequential and multicore options.

If your jobs in these exercises keep running longer than 10 minutes, something is probably wrong - use `scancel <job_id>` to cancel again, check what could be wrong and try again.

Note that `plan(multicore`) does not work in RStudio. If you want to try this example in RStudio, use `plan(multisession)` instead.


### Ex 18: Multiple nodes with `future`

When the resources on one node are not anymore sufficient for a job, it is possible to use multiple nodes. This is a more advanced example compared to the ones above - it is not that easy to make this type of jobs work correctly in R. Because we are distributing the job over several nodes, specific packages are needed to handle the communication between the nodes. Here we are again using the `future` package and the `furr` package in the future package family (`furrr` calls `future` in the background).

``` r
library(furrr) # one package of the future family of packages

# function that carries out the same ordination we used earlier
ordination_function <- function(comm_csv) {
  comm <- read.csv2(comm_csv, row.names = 1)
  nmds <- vegan::metaMDS(comm, trace = FALSE)
  Sys.sleep(5) # added to extend the running time of the small example
  stress_values <- nmds$stress
}

# listing the csv files in the folder communities
comm_csv_list <- list.files(path = "/scratch/project_2016453/<your folder here>/communities", pattern = ".csv", full.names = TRUE) 

# part spefic to multinode jobs starts 
cl <- getMPIcluster()
plan(cluster, workers = cl)

multinode_results <- system.time(future_map(comm_csv_list, ordination_function))
print(multinode_results)

# what does this do?
print(Sys.getenv("SLURM_NODELIST"))

stopCluster(cl)
```

Batch job script (note the partition, the lines for nodes, ntasks-per-node, and the modifications on the last line):

``` bash
#!/bin/bash -l
#SBATCH --job-name=future_map # give your job a name here
#SBATCH --account=project_2016453 # project number of the course project
#SBATCH --output=output_%j.txt
#SBATCH --error=errors_%j.txt
#SBATCH --partition=large # different partition to use multiple nodes
#SBATCH --time=00:05:00 # h:min:sek, this reserves 5 minutes
#SBATCH --nodes=3
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=2
#SBATCH --mem-per-cpu=1000

# Load r-env
module load r-env

# Clean up .Renviron file in home directory
if test -f ~/.Renviron; then
    sed -i '/TMPDIR/d' ~/.Renviron
fi

# Specify a temp folder path (add your personal folder here)
echo "TMPDIR=/scratch/project_2016453/<add your folder here>" >> ~/.Renviron 

# Run the R script - note that this line is different from the other examples
srun apptainer_wrapper exec RMPISNOW --no-save --slave -f future_cluster.R
```

### Ex 19: Extra: monitoring processes during the job

If you are familiar with Linux commands and batch jobs in general, here is an extra challenge. The aim here is to verify that a parallel job works as intended. Start an R batch job that uses multiple cores (note that the job has to run long enough so that you have time for the next steps). Run `squeue` to see which node your job is running on.

Then, in the terminal, go to the specific node with: `ssh <node_number>`. Then, we can check the processes running for your job with top, htop or pstree (or another command for the same purpose you are familiar with):

``` bash
top -u username
```

``` bash
module load htop
htop -u username
```

``` bash
pstree username -np
```
